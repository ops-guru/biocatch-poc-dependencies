# -- String to partially override template (will maintain the release name)
nameOverride: ""

# -- String to override release name
fullnameOverride: ""

# -- Define type, application / scheduled
templateType: application

# -- Add labels to all the deployed resources
commonLabels: {}

# -- Add annotations to all the deployed resources
commonAnnotations: {}

# -- Define application name
name: ""

# -- Add labels to application
labels: {}
# -- Add annotations to application
annotations: {}

# -- SparkApplication API docs
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkapplication
# Follow if templateType: application

# -- Type tells the type of the Spark application.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.SparkApplicationType
type: Scala

# -- SparkVersion is the version of Spark the application uses.
sparkVersion: ""

# -- Mode is the deployment mode of the Spark application.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.DeployMode
mode: cluster

# -- (Optional) ProxyUser specifies the user to impersonate when submitting the application. It maps to the command-line flag “–proxy-user” in spark-submit.
proxyUser: ""

# -- (Optional) Image is the container image for the driver, executor, and init-container. Any custom container images for the driver, executor, or init-container takes precedence over this.
image: ""

# -- (Optional) ImagePullPolicy is the image pull policy for the driver, executor, and init-container.
imagePullPolicy: ""

# -- (Optional) ImagePullSecrets is the list of image-pull secrets.
imagePullSecrets: []

# -- (Optional) MainClass is the fully-qualified main class of the Spark application. This only applies to Java/Scala Spark applications.
mainClass: ""

# -- (Optional) MainFile is the path to a bundled JAR, Python, or R file of the application.
mainApplicationFile: ""

# -- (Optional) Arguments is a list of arguments to be passed to the application.
arguments: []

# -- (Optional) SparkConf carries user-specified Spark configuration properties as they would use the “–conf” option in spark-submit.
sparkConf: {}

# -- (Optional) HadoopConf carries user-specified Hadoop configuration properties as they would use the the “–conf” option in spark-submit. The SparkApplication controller automatically adds prefix “spark.hadoop.” to Hadoop configuration properties.
hadoopConf: {}

# -- (Optional) SparkConfigMap carries the name of the ConfigMap containing Spark configuration files such as log4j.properties. The controller will add environment variable SPARK_CONF_DIR to the path where the ConfigMap is mounted to.
sparkConfigMap: ""

# -- (Optional) HadoopConfigMap carries the name of the ConfigMap containing Hadoop configuration files such as core-site.xml. The controller will add environment variable HADOOP_CONF_DIR to the path where the ConfigMap is mounted to.
hadoopConfigMap: ""

# -- (Optional) Volumes is the list of Kubernetes volumes that can be mounted by the driver and/or executors.
# https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#volume-v1-core
volumes: []

# -- Driver is the driver specification.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.DriverSpec
driver: {}

# -- Executor is the executor specification.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.ExecutorSpec
executor: {}

# -- (Optional) Deps captures all possible types of dependencies of a Spark application.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.Dependencies
deps: {}

# -- RestartPolicy defines the policy on if and in which conditions the controller should restart an application.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.RestartPolicy
restartPolicy: {}

# -- (Optional) NodeSelector is the Kubernetes node selector to be added to the driver and executor pods. This field is mutually exclusive with nodeSelector at podSpec level (driver or executor). This field will be deprecated in future versions (at SparkApplicationSpec level).
nodeSelector: {}

# -- (Optional) FailureRetries is the number of times to retry a failed application before giving up. This is best effort and actual retry attempts can be >= the value specified.
failureRetries: ""

# -- (Optional) RetryInterval is the unit of intervals in seconds between submission retries.
retryInterval: ""

# -- (Optional) This sets the major Python version of the docker image used to run the driver and executor containers. Can either be 2 or 3, default 2.
pythonVersion: ""

# -- (Optional) This sets the Memory Overhead Factor that will allocate memory to non-JVM memory. For JVM-based jobs this value will default to 0.10, for non-JVM jobs 0.40. Value of this field will be overridden by Spec.Driver.MemoryOverhead and Spec.Executor.MemoryOverhead if they are set.
memoryOverheadFactor: ""

# -- (Optional) Monitoring configures how monitoring is handled.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.MonitoringSpec
monitoring: {}

# -- (Optional) BatchScheduler configures which batch scheduler will be used for scheduling
batchScheduler: ""

# -- (Optional) TimeToLiveSeconds defines the Time-To-Live (TTL) duration in seconds for this SparkApplication after its termination. The SparkApplication object will be garbage collected if the current time is more than the TimeToLiveSeconds since its termination.
timeToLiveSeconds: ""

# -- (Optional) BatchSchedulerOptions provides fine-grained control on how to batch scheduling.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.BatchSchedulerConfiguration
batchSchedulerOptions: {}

# -- (Optional) SparkUIOptions allows configuring the Service and the Ingress to expose the sparkUI
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.SparkUIConfiguration
sparkUIOptions: {}

# -- (Optional) DynamicAllocation configures dynamic allocation that becomes available for the Kubernetes scheduler backend since Spark 3.0.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.DynamicAllocation
dynamicAllocation: {}

# -- ScheduledSparkApplication API docs
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#scheduledsparkapplication
# -- Follow if templateType: scheduled


# -- Schedule is a cron schedule on which the application should run.
schedule: ""

# -- (Optional) Suspend is a flag telling the controller to suspend subsequent runs of the application if set to true. Defaults to false.
suspend: false

# -- ConcurrencyPolicy is the policy governing concurrent SparkApplication runs.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.ConcurrencyPolicy
concurrencyPolicy: ""

# -- SuccessfulRunHistoryLimit is the number of past successful runs of the application to keep. Defaults to 1.
successfulRunHistoryLimit: 1

# -- FailedRunHistoryLimit is the number of past failed runs of the application to keep. Defaults to 1.
failedRunHistoryLimit: 1

# -- Template is a template from which SparkApplication instances can be created.
# https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api-docs.md#sparkoperator.k8s.io/v1beta2.SparkApplicationSpec
template: {}

# -- Role Based Access
# Ref: https://kubernetes.io/docs/admin/authorization/rbac/
rbac:
  create: true
  # -- Create Role or RoleBinding
  clusterWideAccess: true
  # -- Additional rbac rules
  rules: []

# -- Service account for pods to use
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
serviceAccount:
  # -- Enable creation of ServiceAccount for pods
  create: true
  # -- The name of the ServiceAccount to use.
  # -- If not set and create is true, a driver.serviceAccount is used. If not set as well, a chart name is used
  name: ""
  # -- Allows auto mount of ServiceAccountToken on the serviceAccount created
  # -- Can be set to false if pods using this serviceAccount do not need to use K8s API
  automountServiceAccountToken: true
  # -- Additional custom annotations for the ServiceAccount
  annotations: {}